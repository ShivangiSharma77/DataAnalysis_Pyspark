# -*- coding: utf-8 -*-
"""Machine Learning_Pyspark .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L66i3pdp2K3gkOE_RbihM9Tu82ch15yZ

## Spark installation
"""

!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz
!tar -xvf spark-3.1.1-bin-hadoop2.7.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop2.7"
import findspark
findspark.init()
from google.colab import files
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.sql.functions import isnan, when, count, col, lit
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml import Pipeline
from pyspark.ml.tuning import CrossValidator
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.tuning import ParamGridBuilder

sc = SparkSession.builder.master("local[*]").getOrCreate()

"""# Data loading and cleanig"""

files.upload()
# my_data.printSchema()

!ls

!cat data.csv

data= sc.read.csv("data.csv", inferSchema=True, header=True)

data.head

data.printSchema()
data.describe().toPandas().transpose()

def replace(column, value):
  return when(column != value, column).otherwise(lit(None))
data = data.withColumn("Market Category", replace(col("Market Category"), "N/A"))

data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns]).show()

data = data.drop("Market Category")
data = data.na.drop()
print((data.count(), len(data.columns)))

data.columns

"""# Building pipeline"""

assembler = VectorAssembler(inputCols=["Year", 'Engine HP','Engine Cylinders','Number of Doors','highway MPG','city mpg','Popularity'], outputCol = "Attributes")
regressor = RandomForestRegressor(featuresCol = "Attributes", labelCol="MSRP")
pipeline = Pipeline(stages=[assembler, regressor])
pipeline.write().overwrite().save("pipeline")

!ls

"""# HyperTuning and cross validation"""

pipelineModel = Pipeline.load("pipeline")
paramGrid = ParamGridBuilder().addGrid(regressor.numTrees, [100, 500]).build()
crossval = CrossValidator(estimator=pipelineModel,
                          estimatorParamMaps = paramGrid,
                          evaluator= RegressionEvaluator(labelCol = "MSRP"),
                          numFolds=3)

"""# Training and Prediction"""

train_data, test_data = data.randomSplit([0.8,0.2], seed=123)
cvModel= crossval.fit(train_data)

bestModel= cvModel.bestModel
for x in range(len(bestModel.stages)):
  print(bestModel.stages[x])

pred = cvModel.transform(test_data)
pred.select("MSRP", "prediction").show()

"""# Evaluation """

eval = RegressionEvaluator(labelCol = "MSRP")
rmse = eval.evaluate(pred)
mse= eval.evaluate(pred, {eval.metricName: "mse"})
mae= eval.evaluate(pred, {eval.metricName: "mae"})
r2 = eval.evaluate(pred, {eval.metricName: "r2"})

print("RMSE: %.3f" %rmse)
print("MSE: %.3f" %mse)
print("MAE: %.3f" %mae)
print("r2: %.3f" %r2)

